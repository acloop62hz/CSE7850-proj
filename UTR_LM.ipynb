{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KUlH1wT62uB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cec3c29-6507-447c-d9c9-1854cc79e9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fair-esm in /usr/local/lib/python3.11/dist-packages (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import esm\n",
        "from typing import Sequence, Tuple, List, Union\n",
        "import os"
      ],
      "metadata": {
        "id": "isHXo7IxTIoF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61CWPlmgJz11",
        "outputId": "4f37a310-14cc-4de0-cbfa-a9d88f0e8d43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "MqZ6y1VF-DA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FastaBatchedDataset(object):\n",
        "    def __init__(self, labels, sequences):\n",
        "        self.labels = list(labels)\n",
        "        self.sequences = list(sequences)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.labels[idx], self.sequences[idx]"
      ],
      "metadata": {
        "id": "j1OLjdPbtPI1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchConverter(object):\n",
        "    \"\"\"Callable to convert an unprocessed (labels + strings) batch to a\n",
        "    processed (labels + tensor) batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alphabet, truncation_seq_length: int = None):\n",
        "        self.alphabet = alphabet\n",
        "        self.truncation_seq_length = truncation_seq_length\n",
        "\n",
        "    def __call__(self, raw_batch: Sequence[Tuple[str, str]]):\n",
        "        # RoBERTa uses an eos token, while ESM-1 does not.\n",
        "        batch_size = len(raw_batch)\n",
        "        batch_labels, seq_str_list = zip(*raw_batch)\n",
        "        seq_encoded_list = [self.alphabet.encode(seq_str) for seq_str in seq_str_list]\n",
        "        if self.truncation_seq_length:\n",
        "            seq_encoded_list = [seq_str[:self.truncation_seq_length] for seq_str in seq_encoded_list]\n",
        "        max_len = max(len(seq_encoded) for seq_encoded in seq_encoded_list)\n",
        "        tokens = torch.empty(\n",
        "            (\n",
        "                batch_size,\n",
        "                max_len + int(self.alphabet.prepend_bos) + int(self.alphabet.append_eos),\n",
        "            ),\n",
        "            dtype=torch.int64,\n",
        "        )\n",
        "        tokens.fill_(self.alphabet.padding_idx)\n",
        "        labels = []\n",
        "\n",
        "        for i, (label, seq_str, seq_encoded) in enumerate(\n",
        "            zip(batch_labels, seq_str_list, seq_encoded_list)\n",
        "        ):\n",
        "            labels.append(label)\n",
        "            if self.alphabet.prepend_bos:\n",
        "                tokens[i, 0] = self.alphabet.cls_idx\n",
        "            seq = torch.tensor(seq_encoded, dtype=torch.int64)\n",
        "            tokens[\n",
        "                i,\n",
        "                int(self.alphabet.prepend_bos) : len(seq_encoded)\n",
        "                + int(self.alphabet.prepend_bos),\n",
        "            ] = seq\n",
        "            if self.alphabet.append_eos:\n",
        "                tokens[i, len(seq_encoded) + int(self.alphabet.prepend_bos)] = self.alphabet.eos_idx\n",
        "\n",
        "        return tokens, labels"
      ],
      "metadata": {
        "id": "zy0Nvoepijd_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Alphabet(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        standard_toks: Sequence[str],\n",
        "        prepend_toks: Sequence[str] = (\"<pad>\", \"<eos>\", \"<unk>\"),\n",
        "        append_toks: Sequence[str] = (\"<cls>\", \"<mask>\", \"<sep>\"),\n",
        "        prepend_bos: bool = True,\n",
        "        append_eos: bool = True,\n",
        "    ):\n",
        "        self.standard_toks = list(standard_toks)\n",
        "        self.prepend_toks = list(prepend_toks)\n",
        "        self.append_toks = list(append_toks)\n",
        "        self.prepend_bos = prepend_bos\n",
        "        self.append_eos = append_eos\n",
        "\n",
        "        self.all_toks = list(self.prepend_toks)\n",
        "        self.all_toks.extend(self.standard_toks)\n",
        "        self.all_toks.extend(self.append_toks)\n",
        "\n",
        "        self.tok_to_idx = {tok: i for i, tok in enumerate(self.all_toks)}\n",
        "        self.unk_idx = self.tok_to_idx[\"<unk>\"]\n",
        "        self.padding_idx = self.get_idx(\"<pad>\")\n",
        "        self.cls_idx = self.get_idx(\"<cls>\")\n",
        "        self.mask_idx = self.get_idx(\"<mask>\")\n",
        "        self.eos_idx = self.get_idx(\"<eos>\")\n",
        "        self.all_special_tokens = ['<eos>', '<pad>', '<mask>']\n",
        "        self.unique_no_split_tokens = self.all_toks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_toks)\n",
        "\n",
        "    def get_idx(self, tok):\n",
        "        return self.tok_to_idx.get(tok, self.unk_idx)\n",
        "\n",
        "    def get_tok(self, ind):\n",
        "        return self.all_toks[ind]\n",
        "\n",
        "    def to_dict(self):\n",
        "        return self.tok_to_idx.copy()\n",
        "\n",
        "    def get_batch_converter(self):\n",
        "        return BatchConverter(self)\n",
        "\n",
        "    def _tokenize(self, text) -> str:\n",
        "        return text.split()\n",
        "\n",
        "    def tokenize(self, text: str, **kwargs) -> List[str]:\n",
        "        tokens = []\n",
        "        i = 0\n",
        "        while i < len(text):\n",
        "            if text[i] == '<':\n",
        "                j = text.find('>', i)\n",
        "                if j == -1:\n",
        "                    raise ValueError(f\"Unclosed special token starting at position {i}: {text[i:i+10]}\")\n",
        "                tokens.append(text[i:j+1])\n",
        "                i = j + 1\n",
        "            else:\n",
        "                tokens.append(text[i])\n",
        "                i += 1\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.tok_to_idx[tok] for tok in self.tokenize(text)]"
      ],
      "metadata": {
        "id": "-MYq8KiXiK4x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Union\n",
        "\n",
        "from esm.modules import ESM1bLayerNorm, TransformerLayer, RobertaLMHead\n",
        "\n",
        "class ESM2SI(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int = 33,\n",
        "        embed_dim: int = 1280,\n",
        "        attention_heads: int = 20,\n",
        "        alphabet: any = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.embed_dim = embed_dim\n",
        "        self.attention_heads = attention_heads\n",
        "\n",
        "        self.alphabet = alphabet\n",
        "        self.alphabet_size = len(alphabet)\n",
        "        self.padding_idx = alphabet.padding_idx\n",
        "        self.cls_idx = alphabet.cls_idx\n",
        "        self.eos_idx = alphabet.eos_idx\n",
        "        self.prepend_bos = alphabet.prepend_bos\n",
        "        self.append_eos = alphabet.append_eos\n",
        "\n",
        "        self._init_submodules()\n",
        "\n",
        "    def _init_submodules(self):\n",
        "        self.embed_scale = 1\n",
        "        self.embed_tokens = nn.Embedding(\n",
        "            self.alphabet_size,\n",
        "            self.embed_dim,\n",
        "            padding_idx=self.padding_idx,\n",
        "        )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerLayer(\n",
        "                    self.embed_dim,\n",
        "                    4 * self.embed_dim,\n",
        "                    self.attention_heads,\n",
        "                    add_bias_kv=False,\n",
        "                    use_esm1b_layer_norm=True,\n",
        "                    use_rotary_embeddings=True,\n",
        "                )\n",
        "                for _ in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.emb_layer_norm_after = ESM1bLayerNorm(self.embed_dim)\n",
        "\n",
        "        self.lm_head = RobertaLMHead(\n",
        "            embed_dim=self.embed_dim,\n",
        "            output_dim=self.alphabet_size,\n",
        "            weight=self.embed_tokens.weight,\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens, repr_layers=[], return_representation=True):\n",
        "        assert tokens.ndim == 2\n",
        "        padding_mask = tokens.eq(self.padding_idx)\n",
        "\n",
        "        x = self.embed_scale * self.embed_tokens(tokens)\n",
        "\n",
        "        # Apply padding mask\n",
        "        if padding_mask is not None:\n",
        "            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))\n",
        "\n",
        "        repr_layers = set(repr_layers)\n",
        "        hidden_representations = {}\n",
        "        if 0 in repr_layers:\n",
        "            hidden_representations[0] = x\n",
        "\n",
        "        x = x.transpose(0, 1)  # (B, T, E) -> (T, B, E)\n",
        "\n",
        "        if not padding_mask.any():\n",
        "            padding_mask = None\n",
        "\n",
        "        for layer_idx, layer in enumerate(self.layers):\n",
        "            x, _ = layer(\n",
        "                x,\n",
        "                self_attn_padding_mask=padding_mask,\n",
        "                need_head_weights=False,\n",
        "            )\n",
        "            if (layer_idx + 1) in repr_layers:\n",
        "                hidden_representations[layer_idx + 1] = x.transpose(0, 1)\n",
        "\n",
        "        x = self.emb_layer_norm_after(x)\n",
        "        x = x.transpose(0, 1)  # (T, B, E) -> (B, T, E)\n",
        "\n",
        "        if (layer_idx + 1) in repr_layers:\n",
        "            hidden_representations[layer_idx + 1] = x\n",
        "        x = self.lm_head(x)\n",
        "\n",
        "        if return_representation:\n",
        "            return {\"logits\": x,\"representations\": hidden_representations}\n",
        "        else:\n",
        "            return {}"
      ],
      "metadata": {
        "id": "PKxsQbLn1HZ4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ESM2_Linear(nn.Module):\n",
        "    def __init__(self, embed_dim, nodes, projection_dim=64):\n",
        "        super().__init__()\n",
        "        self.esm2 = ESM2SI(num_layers = 6,\n",
        "              embed_dim = 128,\n",
        "              attention_heads = 16,\n",
        "              alphabet = alphabet)\n",
        "\n",
        "        self.nodes = nodes\n",
        "        self.dropout3 = 0.2\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, self.nodes)\n",
        "        self.dropout3 = nn.Dropout(self.dropout3)\n",
        "\n",
        "        self.output = nn.Linear(self.nodes, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, tokens, repr_layer=6, return_representation=True):\n",
        "        x = self.esm2(tokens, repr_layers=[repr_layer], return_representation=return_representation)\n",
        "        logits = x[\"logits\"]\n",
        "\n",
        "        # get representation\n",
        "        x = x[\"representations\"][repr_layer][:, 0]\n",
        "        x_o = x.unsqueeze(2)\n",
        "        x = self.flatten(x_o)\n",
        "\n",
        "\n",
        "        # prediction head\n",
        "        x = self.fc(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x, logits\n"
      ],
      "metadata": {
        "id": "64ov_YvH4Yvv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "id": "huSgjGRF-Lrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np\n",
        "\n",
        "def con_fit_loss(pred, labels, logits_new, logits_old=None, lambda_kl=0.0):\n",
        "\n",
        "    # 1. Pairwise Ranking Loss\n",
        "    pairwise_loss = 0.0\n",
        "    n = pred.size(0)\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if labels[i] > labels[j]:\n",
        "                diff = pred[i] - pred[j]\n",
        "                pairwise_loss += F.softplus(-diff)  # log(1+exp(-diff))\n",
        "\n",
        "    pairwise_loss = pairwise_loss / (n * n)\n",
        "\n",
        "    # 2. KL Divergence Loss\n",
        "    if logits_old is not None and lambda_kl > 0.0:\n",
        "        logits_new_cls = logits_new[:, 0, :]  # (batch, vocab_size)\n",
        "        logits_old_cls = logits_old[:, 0, :]\n",
        "\n",
        "        p_new = F.log_softmax(logits_new_cls, dim=-1)\n",
        "        p_old = F.softmax(logits_old_cls, dim=-1)\n",
        "        kl_loss = F.kl_div(p_new, p_old, reduction='batchmean')\n",
        "    else:\n",
        "        kl_loss = torch.tensor(0.0, device=pred.device)\n",
        "\n",
        "    total_loss = pairwise_loss + lambda_kl * kl_loss\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def pairwise_ranking_loss(preds, targets, margin=1.0):\n",
        "    \"\"\"\n",
        "    preds: Tensor of shape (B, 1) – predicted DMS scores\n",
        "    targets: Tensor of shape (B, 1) – true DMS scores\n",
        "    \"\"\"\n",
        "    preds = preds.squeeze()\n",
        "    targets = targets.squeeze()\n",
        "    diff_target = targets.unsqueeze(0) - targets.unsqueeze(1)\n",
        "    diff_pred = preds.unsqueeze(0) - preds.unsqueeze(1)\n",
        "    target_sign = torch.sign(diff_target)\n",
        "    loss_matrix = F.relu(-target_sign * diff_pred + margin)\n",
        "    mask = torch.eye(len(targets), dtype=torch.bool, device=targets.device)\n",
        "    loss_matrix = loss_matrix[~mask].view(len(targets), -1)\n",
        "    return loss_matrix.mean()"
      ],
      "metadata": {
        "id": "qlu7SpF8C5E9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Config"
      ],
      "metadata": {
        "id": "1qlAmU_A-NNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = Alphabet(standard_toks = 'AGCT')\n",
        "modelfile = '/content/ESM2SI_3.1_fiveSpeciesCao_6layers_16heads_128embedsize_4096batchToks_MLMLossMin.pkl'\n",
        "obj_col = 'te_log'\n",
        "seq_type = 'utr'\n",
        "config = {\n",
        "    \"num_epochs\": 20,\n",
        "    \"early_stop_patience\": 5,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate_esm\": 1e-5,\n",
        "    \"learning_rate_head\": 1e-3,\n",
        "    \"nodes\": 40,\n",
        "    \"save_dir\": \"/content/saved_models\"\n",
        "}"
      ],
      "metadata": {
        "id": "QTDkLEUa_PqN"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read in Data"
      ],
      "metadata": {
        "id": "cI5Evkjr-RBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/pc3_sequence.csv')\n",
        "dataset = FastaBatchedDataset(data.loc[:,obj_col], data[seq_type])"
      ],
      "metadata": {
        "id": "1Rf0SrENztE_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting optimizer and scheduler"
      ],
      "metadata": {
        "id": "j90LZLsq-TNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "def setup_optimizer_and_scheduler(model, train_loader, epochs, batch_size, lr_for_esm=1e-6, lr_for_head=1e-3):\n",
        "    \"\"\"\n",
        "    Set up optimizer and scheduler for ESM2 finetuning model.\n",
        "\n",
        "    Args:\n",
        "        model: The model containing esm2 encoder and regression head (like fc/output layers).\n",
        "        train_loader: DataLoader for training set.\n",
        "        epochs: Total number of epochs.\n",
        "        batch_size: Training batch size.\n",
        "        lr_for_esm: Learning rate for pretrained ESM2 encoder.\n",
        "        lr_for_head: Learning rate for new prediction head.\n",
        "\n",
        "    Returns:\n",
        "        optimizer, scheduler\n",
        "    \"\"\"\n",
        "\n",
        "    # Define scaled learning rates\n",
        "    lr_esm = batch_size * lr_for_esm\n",
        "    lr_head = batch_size * lr_for_head\n",
        "\n",
        "    # Collect parameters\n",
        "    esm2_params = list(model.esm2.parameters())\n",
        "    head_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not name.startswith(\"esm2.\"):  # Everything not inside esm2 is considered head\n",
        "            head_params.append(param)\n",
        "\n",
        "    # Group parameters\n",
        "    param_groups = [\n",
        "        {'params': esm2_params, 'lr': lr_esm},\n",
        "        {'params': head_params, 'lr': lr_head}\n",
        "    ]\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = AdamW(param_groups, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
        "\n",
        "    # Initialize scheduler\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=[lr_esm, lr_head],\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=epochs,\n",
        "    )\n",
        "\n",
        "    return optimizer, scheduler\n"
      ],
      "metadata": {
        "id": "KJbCOYLYTSlU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_model = ESM2_Linear(128, config[\"nodes\"]).to(device)\n",
        "state_dict = torch.load(modelfile, map_location=device)\n",
        "new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "old_model.esm2.load_state_dict(new_state_dict, strict=False)\n",
        "for param in old_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "APYiixMZI_uc"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "rIaq3d95-cck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "\n",
        "scaler = GradScaler('cuda')\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scheduler, criterion, device):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            optimizer.zero_grad()\n",
        "            predictions, logits = model(inputs)\n",
        "            if criterion == \"btr\":\n",
        "                with torch.no_grad():\n",
        "                    _, logits_old = old_model(inputs)\n",
        "                loss = con_fit_loss(predictions, labels, logits, logits_old, lambda_kl=0.1)\n",
        "            elif criterion == 'pairwise':\n",
        "                loss = pairwise_ranking_loss(predictions, labels)\n",
        "            else:\n",
        "                loss = criterion(predictions, labels.view(-1, 1))\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "            with autocast('cuda'):\n",
        "                predictions, logits = model(inputs)\n",
        "                if criterion == \"btr\":\n",
        "                    with torch.no_grad():\n",
        "                        _, logits_old = old_model(inputs)\n",
        "                    loss = con_fit_loss(predictions, labels, logits, logits_old, lambda_kl=0.1)\n",
        "                elif criterion == 'pairwise':\n",
        "                    loss = pairwise_ranking_loss(predictions, labels)\n",
        "                else:\n",
        "                    loss = criterion(predictions, labels.view(-1, 1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.squeeze(all_preds)\n",
        "    all_labels = np.squeeze(all_labels)\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        f\"loss\": total_loss / len(loader),\n",
        "        f\"spearman\": spearmanr(all_labels, all_preds)[0],\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "sFBS0ii7Dcc1"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "gkf = GroupKFold(n_splits=n_splits)\n",
        "group_labels = data['external_gene_id']\n",
        "fold_spearman_scores = []\n",
        "\n",
        "\n",
        "# for fold, (train_idx, val_idx) in enumerate(gkf.split(dataset, groups=group_labels)):\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
        "    print(f\"\\n=== Fold {fold+1}/{n_splits} ===\")\n",
        "\n",
        "    # new dataloader\n",
        "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "    train_dataloader = DataLoader(train_subset,\n",
        "                                        batch_size= config[\"batch_size\"],\n",
        "                                        shuffle = True,\n",
        "                                        collate_fn=alphabet.get_batch_converter())\n",
        "    val_dataloader = DataLoader(val_subset,\n",
        "                                        batch_size= config[\"batch_size\"],\n",
        "                                        shuffle = False,\n",
        "                                        collate_fn=alphabet.get_batch_converter())\n",
        "\n",
        "    # initialize model/optimizer\n",
        "    model = ESM2_Linear(128, config[\"nodes\"]).to(device)\n",
        "    state_dict = torch.load(modelfile, map_location=device)\n",
        "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "    model.esm2.load_state_dict(new_state_dict, strict=False)\n",
        "    optimizer, scheduler = setup_optimizer_and_scheduler(model, train_dataloader, config[\"num_epochs\"], config[\"batch_size\"], config[\"learning_rate_esm\"], config[\"learning_rate_head\"])\n",
        "    # criterion = nn.MSELoss()\n",
        "    # criterion = torch.nn.HuberLoss()\n",
        "    criterion = \"pairwise\"\n",
        "    # criterion = \"btr\"\n",
        "\n",
        "    best_val_metric = -float('inf')\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        train_loss = train_one_epoch(model, train_dataloader, optimizer, scheduler, criterion, device)\n",
        "        val_metric = evaluate(model, val_dataloader, criterion, device)\n",
        "        print(f\"[Fold {fold+1} | Epoch {epoch}] Train Loss: {train_loss:.4f} | Val Loss: {val_metric['loss']:.4f} | Val Spearman: {val_metric['spearman']:.4f}\")\n",
        "\n",
        "        if val_metric['spearman'] > best_val_metric:\n",
        "            best_val_metric = val_metric['spearman']\n",
        "            epochs_since_improvement = 0\n",
        "            # save model\n",
        "            torch.save(model.state_dict(), os.path.join(config[\"save_dir\"], f\"best_model_fold{fold+1}.pt\"))\n",
        "            print(f\"New best Spearman: {best_val_metric:.4f} — Model saved.\")\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "\n",
        "        if epochs_since_improvement >= config[\"early_stop_patience\"]:\n",
        "            print(f\"Early stopping triggered in fold {fold+1} after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Best Spearman for fold {fold+1}: {best_val_metric:.4f}\")\n",
        "    fold_spearman_scores.append(best_val_metric)\n",
        "\n",
        "# avergae spearman\n",
        "mean_spearman = sum(fold_spearman_scores) / len(fold_spearman_scores)\n",
        "print(f\"\\n5-Fold Cross-Validation Mean Spearman: {mean_spearman:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bms0mBmrqaN",
        "outputId": "8df83166-d8d8-464b-b565-6937287a8077"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 1/5 ===\n",
            "[Fold 1 | Epoch 0] Train Loss: 0.9317 | Val Loss: 1.0317 | Val Spearman: 0.2062\n",
            "New best Spearman: 0.2062 — Model saved.\n",
            "[Fold 1 | Epoch 1] Train Loss: 0.8863 | Val Loss: 1.0078 | Val Spearman: 0.2739\n",
            "New best Spearman: 0.2739 — Model saved.\n",
            "[Fold 1 | Epoch 2] Train Loss: 0.8750 | Val Loss: 1.0148 | Val Spearman: 0.3000\n",
            "New best Spearman: 0.3000 — Model saved.\n",
            "[Fold 1 | Epoch 3] Train Loss: 0.8472 | Val Loss: 1.0180 | Val Spearman: 0.3360\n",
            "New best Spearman: 0.3360 — Model saved.\n",
            "[Fold 1 | Epoch 4] Train Loss: 0.8139 | Val Loss: 1.0236 | Val Spearman: 0.3796\n",
            "New best Spearman: 0.3796 — Model saved.\n",
            "[Fold 1 | Epoch 5] Train Loss: 0.7692 | Val Loss: 1.0206 | Val Spearman: 0.4190\n",
            "New best Spearman: 0.4190 — Model saved.\n",
            "[Fold 1 | Epoch 6] Train Loss: 0.7574 | Val Loss: 1.0300 | Val Spearman: 0.4327\n",
            "New best Spearman: 0.4327 — Model saved.\n",
            "[Fold 1 | Epoch 7] Train Loss: 0.6801 | Val Loss: 1.0513 | Val Spearman: 0.4743\n",
            "New best Spearman: 0.4743 — Model saved.\n",
            "[Fold 1 | Epoch 8] Train Loss: 0.6263 | Val Loss: 1.1436 | Val Spearman: 0.5143\n",
            "New best Spearman: 0.5143 — Model saved.\n",
            "[Fold 1 | Epoch 9] Train Loss: 0.5856 | Val Loss: 1.1727 | Val Spearman: 0.5269\n",
            "New best Spearman: 0.5269 — Model saved.\n",
            "[Fold 1 | Epoch 10] Train Loss: 0.5567 | Val Loss: 1.1360 | Val Spearman: 0.5389\n",
            "New best Spearman: 0.5389 — Model saved.\n",
            "[Fold 1 | Epoch 11] Train Loss: 0.5238 | Val Loss: 1.1272 | Val Spearman: 0.5475\n",
            "New best Spearman: 0.5475 — Model saved.\n",
            "[Fold 1 | Epoch 12] Train Loss: 0.5038 | Val Loss: 1.1093 | Val Spearman: 0.5482\n",
            "New best Spearman: 0.5482 — Model saved.\n",
            "[Fold 1 | Epoch 13] Train Loss: 0.4821 | Val Loss: 1.1307 | Val Spearman: 0.5585\n",
            "New best Spearman: 0.5585 — Model saved.\n",
            "[Fold 1 | Epoch 14] Train Loss: 0.4615 | Val Loss: 1.2105 | Val Spearman: 0.5586\n",
            "New best Spearman: 0.5586 — Model saved.\n",
            "[Fold 1 | Epoch 15] Train Loss: 0.4321 | Val Loss: 1.2489 | Val Spearman: 0.5608\n",
            "New best Spearman: 0.5608 — Model saved.\n",
            "[Fold 1 | Epoch 16] Train Loss: 0.4210 | Val Loss: 1.2178 | Val Spearman: 0.5596\n",
            "[Fold 1 | Epoch 17] Train Loss: 0.4219 | Val Loss: 1.2234 | Val Spearman: 0.5619\n",
            "New best Spearman: 0.5619 — Model saved.\n",
            "[Fold 1 | Epoch 18] Train Loss: 0.4130 | Val Loss: 1.2268 | Val Spearman: 0.5618\n",
            "[Fold 1 | Epoch 19] Train Loss: 0.4054 | Val Loss: 1.2339 | Val Spearman: 0.5613\n",
            "Best Spearman for fold 1: 0.5619\n",
            "\n",
            "=== Fold 2/5 ===\n",
            "[Fold 2 | Epoch 0] Train Loss: 0.9365 | Val Loss: 1.0219 | Val Spearman: 0.2633\n",
            "New best Spearman: 0.2633 — Model saved.\n",
            "[Fold 2 | Epoch 1] Train Loss: 0.8818 | Val Loss: 1.0237 | Val Spearman: 0.3061\n",
            "New best Spearman: 0.3061 — Model saved.\n",
            "[Fold 2 | Epoch 2] Train Loss: 0.8663 | Val Loss: 1.0366 | Val Spearman: 0.3063\n",
            "New best Spearman: 0.3063 — Model saved.\n",
            "[Fold 2 | Epoch 3] Train Loss: 0.8439 | Val Loss: 1.0446 | Val Spearman: 0.3511\n",
            "New best Spearman: 0.3511 — Model saved.\n",
            "[Fold 2 | Epoch 4] Train Loss: 0.8211 | Val Loss: 1.0280 | Val Spearman: 0.3825\n",
            "New best Spearman: 0.3825 — Model saved.\n",
            "[Fold 2 | Epoch 5] Train Loss: 0.7999 | Val Loss: 0.9949 | Val Spearman: 0.3819\n",
            "[Fold 2 | Epoch 6] Train Loss: 0.8203 | Val Loss: 1.0281 | Val Spearman: 0.4282\n",
            "New best Spearman: 0.4282 — Model saved.\n",
            "[Fold 2 | Epoch 7] Train Loss: 0.7234 | Val Loss: 1.0979 | Val Spearman: 0.4766\n",
            "New best Spearman: 0.4766 — Model saved.\n",
            "[Fold 2 | Epoch 8] Train Loss: 0.6553 | Val Loss: 1.0974 | Val Spearman: 0.5170\n",
            "New best Spearman: 0.5170 — Model saved.\n",
            "[Fold 2 | Epoch 9] Train Loss: 0.5892 | Val Loss: 1.1413 | Val Spearman: 0.5320\n",
            "New best Spearman: 0.5320 — Model saved.\n",
            "[Fold 2 | Epoch 10] Train Loss: 0.5944 | Val Loss: 1.1623 | Val Spearman: 0.5306\n",
            "[Fold 2 | Epoch 11] Train Loss: 0.5573 | Val Loss: 1.1586 | Val Spearman: 0.5414\n",
            "New best Spearman: 0.5414 — Model saved.\n",
            "[Fold 2 | Epoch 12] Train Loss: 0.5205 | Val Loss: 1.1717 | Val Spearman: 0.5519\n",
            "New best Spearman: 0.5519 — Model saved.\n",
            "[Fold 2 | Epoch 13] Train Loss: 0.4811 | Val Loss: 1.2104 | Val Spearman: 0.5570\n",
            "New best Spearman: 0.5570 — Model saved.\n",
            "[Fold 2 | Epoch 14] Train Loss: 0.4652 | Val Loss: 1.2661 | Val Spearman: 0.5600\n",
            "New best Spearman: 0.5600 — Model saved.\n",
            "[Fold 2 | Epoch 15] Train Loss: 0.4652 | Val Loss: 1.2026 | Val Spearman: 0.5647\n",
            "New best Spearman: 0.5647 — Model saved.\n",
            "[Fold 2 | Epoch 16] Train Loss: 0.4494 | Val Loss: 1.2036 | Val Spearman: 0.5656\n",
            "New best Spearman: 0.5656 — Model saved.\n",
            "[Fold 2 | Epoch 17] Train Loss: 0.4371 | Val Loss: 1.2240 | Val Spearman: 0.5653\n",
            "[Fold 2 | Epoch 18] Train Loss: 0.4253 | Val Loss: 1.2301 | Val Spearman: 0.5656\n",
            "New best Spearman: 0.5656 — Model saved.\n",
            "[Fold 2 | Epoch 19] Train Loss: 0.4226 | Val Loss: 1.2373 | Val Spearman: 0.5652\n",
            "Best Spearman for fold 2: 0.5656\n",
            "\n",
            "=== Fold 3/5 ===\n",
            "[Fold 3 | Epoch 0] Train Loss: 0.9227 | Val Loss: 1.0278 | Val Spearman: 0.2588\n",
            "New best Spearman: 0.2588 — Model saved.\n",
            "[Fold 3 | Epoch 1] Train Loss: 0.8721 | Val Loss: 1.0406 | Val Spearman: 0.2658\n",
            "New best Spearman: 0.2658 — Model saved.\n",
            "[Fold 3 | Epoch 2] Train Loss: 0.8573 | Val Loss: 1.0152 | Val Spearman: 0.3238\n",
            "New best Spearman: 0.3238 — Model saved.\n",
            "[Fold 3 | Epoch 3] Train Loss: 0.8135 | Val Loss: 1.0383 | Val Spearman: 0.3605\n",
            "New best Spearman: 0.3605 — Model saved.\n",
            "[Fold 3 | Epoch 4] Train Loss: 0.7739 | Val Loss: 1.0689 | Val Spearman: 0.3943\n",
            "New best Spearman: 0.3943 — Model saved.\n",
            "[Fold 3 | Epoch 5] Train Loss: 0.7216 | Val Loss: 1.0980 | Val Spearman: 0.4318\n",
            "New best Spearman: 0.4318 — Model saved.\n",
            "[Fold 3 | Epoch 6] Train Loss: 0.6913 | Val Loss: 1.0192 | Val Spearman: 0.4387\n",
            "New best Spearman: 0.4387 — Model saved.\n",
            "[Fold 3 | Epoch 7] Train Loss: 0.6739 | Val Loss: 1.1229 | Val Spearman: 0.4816\n",
            "New best Spearman: 0.4816 — Model saved.\n",
            "[Fold 3 | Epoch 8] Train Loss: 0.6190 | Val Loss: 1.1078 | Val Spearman: 0.4997\n",
            "New best Spearman: 0.4997 — Model saved.\n",
            "[Fold 3 | Epoch 9] Train Loss: 0.6319 | Val Loss: 1.0616 | Val Spearman: 0.5009\n",
            "New best Spearman: 0.5009 — Model saved.\n",
            "[Fold 3 | Epoch 10] Train Loss: 0.5731 | Val Loss: 1.1960 | Val Spearman: 0.5302\n",
            "New best Spearman: 0.5302 — Model saved.\n",
            "[Fold 3 | Epoch 11] Train Loss: 0.5323 | Val Loss: 1.0910 | Val Spearman: 0.5477\n",
            "New best Spearman: 0.5477 — Model saved.\n",
            "[Fold 3 | Epoch 12] Train Loss: 0.4774 | Val Loss: 1.2032 | Val Spearman: 0.5491\n",
            "New best Spearman: 0.5491 — Model saved.\n",
            "[Fold 3 | Epoch 13] Train Loss: 0.4622 | Val Loss: 1.1978 | Val Spearman: 0.5544\n",
            "New best Spearman: 0.5544 — Model saved.\n",
            "[Fold 3 | Epoch 14] Train Loss: 0.4466 | Val Loss: 1.1852 | Val Spearman: 0.5552\n",
            "New best Spearman: 0.5552 — Model saved.\n",
            "[Fold 3 | Epoch 15] Train Loss: 0.4263 | Val Loss: 1.2459 | Val Spearman: 0.5582\n",
            "New best Spearman: 0.5582 — Model saved.\n",
            "[Fold 3 | Epoch 16] Train Loss: 0.4189 | Val Loss: 1.2736 | Val Spearman: 0.5580\n",
            "[Fold 3 | Epoch 17] Train Loss: 0.4142 | Val Loss: 1.2450 | Val Spearman: 0.5581\n",
            "[Fold 3 | Epoch 18] Train Loss: 0.4079 | Val Loss: 1.2655 | Val Spearman: 0.5582\n",
            "[Fold 3 | Epoch 19] Train Loss: 0.4073 | Val Loss: 1.2617 | Val Spearman: 0.5578\n",
            "Best Spearman for fold 3: 0.5582\n",
            "\n",
            "=== Fold 4/5 ===\n",
            "[Fold 4 | Epoch 0] Train Loss: 0.9277 | Val Loss: 1.0050 | Val Spearman: 0.2529\n",
            "New best Spearman: 0.2529 — Model saved.\n",
            "[Fold 4 | Epoch 1] Train Loss: 0.8870 | Val Loss: 1.0017 | Val Spearman: 0.2885\n",
            "New best Spearman: 0.2885 — Model saved.\n",
            "[Fold 4 | Epoch 2] Train Loss: 0.8841 | Val Loss: 1.0070 | Val Spearman: 0.3249\n",
            "New best Spearman: 0.3249 — Model saved.\n",
            "[Fold 4 | Epoch 3] Train Loss: 0.8540 | Val Loss: 1.0387 | Val Spearman: 0.3510\n",
            "New best Spearman: 0.3510 — Model saved.\n",
            "[Fold 4 | Epoch 4] Train Loss: 0.8181 | Val Loss: 1.0294 | Val Spearman: 0.4006\n",
            "New best Spearman: 0.4006 — Model saved.\n",
            "[Fold 4 | Epoch 5] Train Loss: 0.7716 | Val Loss: 1.1201 | Val Spearman: 0.4363\n",
            "New best Spearman: 0.4363 — Model saved.\n",
            "[Fold 4 | Epoch 6] Train Loss: 0.7283 | Val Loss: 1.0342 | Val Spearman: 0.4613\n",
            "New best Spearman: 0.4613 — Model saved.\n",
            "[Fold 4 | Epoch 7] Train Loss: 0.6752 | Val Loss: 1.0561 | Val Spearman: 0.4838\n",
            "New best Spearman: 0.4838 — Model saved.\n",
            "[Fold 4 | Epoch 8] Train Loss: 0.6369 | Val Loss: 1.1148 | Val Spearman: 0.5030\n",
            "New best Spearman: 0.5030 — Model saved.\n",
            "[Fold 4 | Epoch 9] Train Loss: 0.6395 | Val Loss: 1.0781 | Val Spearman: 0.5145\n",
            "New best Spearman: 0.5145 — Model saved.\n",
            "[Fold 4 | Epoch 10] Train Loss: 0.5954 | Val Loss: 1.1585 | Val Spearman: 0.5232\n",
            "New best Spearman: 0.5232 — Model saved.\n",
            "[Fold 4 | Epoch 11] Train Loss: 0.5542 | Val Loss: 1.1138 | Val Spearman: 0.5397\n",
            "New best Spearman: 0.5397 — Model saved.\n",
            "[Fold 4 | Epoch 12] Train Loss: 0.5241 | Val Loss: 1.1354 | Val Spearman: 0.5429\n",
            "New best Spearman: 0.5429 — Model saved.\n",
            "[Fold 4 | Epoch 13] Train Loss: 0.4955 | Val Loss: 1.1165 | Val Spearman: 0.5496\n",
            "New best Spearman: 0.5496 — Model saved.\n",
            "[Fold 4 | Epoch 14] Train Loss: 0.4717 | Val Loss: 1.1856 | Val Spearman: 0.5616\n",
            "New best Spearman: 0.5616 — Model saved.\n",
            "[Fold 4 | Epoch 15] Train Loss: 0.4579 | Val Loss: 1.1404 | Val Spearman: 0.5632\n",
            "New best Spearman: 0.5632 — Model saved.\n",
            "[Fold 4 | Epoch 16] Train Loss: 0.4413 | Val Loss: 1.1957 | Val Spearman: 0.5646\n",
            "New best Spearman: 0.5646 — Model saved.\n",
            "[Fold 4 | Epoch 17] Train Loss: 0.4298 | Val Loss: 1.2331 | Val Spearman: 0.5638\n",
            "[Fold 4 | Epoch 18] Train Loss: 0.4201 | Val Loss: 1.2174 | Val Spearman: 0.5641\n",
            "[Fold 4 | Epoch 19] Train Loss: 0.4179 | Val Loss: 1.2326 | Val Spearman: 0.5642\n",
            "Best Spearman for fold 4: 0.5646\n",
            "\n",
            "=== Fold 5/5 ===\n",
            "[Fold 5 | Epoch 0] Train Loss: 0.9306 | Val Loss: 1.0126 | Val Spearman: 0.2431\n",
            "New best Spearman: 0.2431 — Model saved.\n",
            "[Fold 5 | Epoch 1] Train Loss: 0.8880 | Val Loss: 1.0047 | Val Spearman: 0.2721\n",
            "New best Spearman: 0.2721 — Model saved.\n",
            "[Fold 5 | Epoch 2] Train Loss: 0.8799 | Val Loss: 1.0169 | Val Spearman: 0.3201\n",
            "New best Spearman: 0.3201 — Model saved.\n",
            "[Fold 5 | Epoch 3] Train Loss: 0.8613 | Val Loss: 1.0555 | Val Spearman: 0.3408\n",
            "New best Spearman: 0.3408 — Model saved.\n",
            "[Fold 5 | Epoch 4] Train Loss: 0.8217 | Val Loss: 1.0225 | Val Spearman: 0.3972\n",
            "New best Spearman: 0.3972 — Model saved.\n",
            "[Fold 5 | Epoch 5] Train Loss: 0.7709 | Val Loss: 1.0556 | Val Spearman: 0.4451\n",
            "New best Spearman: 0.4451 — Model saved.\n",
            "[Fold 5 | Epoch 6] Train Loss: 0.7462 | Val Loss: 1.0615 | Val Spearman: 0.4551\n",
            "New best Spearman: 0.4551 — Model saved.\n",
            "[Fold 5 | Epoch 7] Train Loss: 0.6923 | Val Loss: 1.0992 | Val Spearman: 0.5014\n",
            "New best Spearman: 0.5014 — Model saved.\n",
            "[Fold 5 | Epoch 8] Train Loss: 0.6376 | Val Loss: 1.0918 | Val Spearman: 0.5144\n",
            "New best Spearman: 0.5144 — Model saved.\n",
            "[Fold 5 | Epoch 9] Train Loss: 0.5673 | Val Loss: 1.1489 | Val Spearman: 0.5409\n",
            "New best Spearman: 0.5409 — Model saved.\n",
            "[Fold 5 | Epoch 10] Train Loss: 0.5362 | Val Loss: 1.1169 | Val Spearman: 0.5495\n",
            "New best Spearman: 0.5495 — Model saved.\n",
            "[Fold 5 | Epoch 11] Train Loss: 0.5147 | Val Loss: 1.1404 | Val Spearman: 0.5578\n",
            "New best Spearman: 0.5578 — Model saved.\n",
            "[Fold 5 | Epoch 12] Train Loss: 0.4915 | Val Loss: 1.1176 | Val Spearman: 0.5602\n",
            "New best Spearman: 0.5602 — Model saved.\n",
            "[Fold 5 | Epoch 13] Train Loss: 0.5027 | Val Loss: 1.1666 | Val Spearman: 0.5619\n",
            "New best Spearman: 0.5619 — Model saved.\n",
            "[Fold 5 | Epoch 14] Train Loss: 0.4789 | Val Loss: 1.2022 | Val Spearman: 0.5650\n",
            "New best Spearman: 0.5650 — Model saved.\n",
            "[Fold 5 | Epoch 15] Train Loss: 0.4542 | Val Loss: 1.2089 | Val Spearman: 0.5706\n",
            "New best Spearman: 0.5706 — Model saved.\n",
            "[Fold 5 | Epoch 16] Train Loss: 0.4309 | Val Loss: 1.2003 | Val Spearman: 0.5725\n",
            "New best Spearman: 0.5725 — Model saved.\n",
            "[Fold 5 | Epoch 17] Train Loss: 0.4253 | Val Loss: 1.2457 | Val Spearman: 0.5721\n",
            "[Fold 5 | Epoch 18] Train Loss: 0.4225 | Val Loss: 1.2337 | Val Spearman: 0.5740\n",
            "New best Spearman: 0.5740 — Model saved.\n",
            "[Fold 5 | Epoch 19] Train Loss: 0.4162 | Val Loss: 1.2428 | Val Spearman: 0.5730\n",
            "Best Spearman for fold 5: 0.5740\n",
            "\n",
            "5-Fold Cross-Validation Mean Spearman: 0.5648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "QKCH7sIz-fbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import rankdata\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# test_data = pd.read_csv('/content/Experimental_data_revised_label.csv')\n",
        "# obj_col = 'label'\n",
        "# seq_type = 'utr_100'\n",
        "\n",
        "test_data = pd.read_csv('/content/HEK_sequence.csv')\n",
        "obj_col = 'te_log'\n",
        "seq_type = 'utr'\n",
        "\n",
        "test_dataset = FastaBatchedDataset(test_data.loc[:, obj_col], test_data[seq_type])\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=alphabet.get_batch_converter()\n",
        ")\n",
        "\n",
        "all_preds = []\n",
        "\n",
        "models = []\n",
        "for fold in range(5):\n",
        "    model =  ESM2_Linear(128, config[\"nodes\"]).to(device)\n",
        "    model_path = os.path.join(config[\"save_dir\"], f\"best_model_fold{fold+1}.pt\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location='cuda'))\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for fold_model in models:\n",
        "        preds = []\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "            with autocast('cuda'):\n",
        "                outputs = model(inputs)[0]\n",
        "\n",
        "            preds.append(outputs.cpu())\n",
        "        preds = torch.cat(preds, dim=0)\n",
        "        all_preds.append(preds)\n",
        "\n",
        "\n",
        "\n",
        "ensemble_preds = torch.stack(all_preds, dim=0).mean(dim=0)\n",
        "\n",
        "y_true = test_data[obj_col].values\n",
        "y_pred = ensemble_preds.numpy().squeeze()\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "spearman_corr = spearmanr(y_true, y_pred).correlation\n",
        "\n",
        "print(f\"Test Spearman of Ensemble Model: {spearman_corr:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek3C1ba0zr2V",
        "outputId": "0d22aadc-d8b3-4eca-fc91-041b881f4f52"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Spearman of Ensemble Model: 0.3517\n",
            "Test Spearman of Ensemble Model (ranked): 0.3517\n"
          ]
        }
      ]
    }
  ]
}