{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KUlH1wT62uB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b18139c-cd8b-48b5-d3c0-287783ef1866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\n",
            "Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import esm\n",
        "from typing import Sequence, Tuple, List, Union"
      ],
      "metadata": {
        "id": "isHXo7IxTIoF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61CWPlmgJz11",
        "outputId": "b750c7de-1a0d-4a9a-c568-2d8184bbd9bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FastaBatchedDataset(object):\n",
        "    def __init__(self, labels, sequences):\n",
        "        self.labels = list(labels)\n",
        "        self.sequences = list(sequences)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.labels[idx], self.sequences[idx]"
      ],
      "metadata": {
        "id": "j1OLjdPbtPI1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchConverter(object):\n",
        "    \"\"\"Callable to convert an unprocessed (labels + strings) batch to a\n",
        "    processed (labels + tensor) batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alphabet, truncation_seq_length: int = None):\n",
        "        self.alphabet = alphabet\n",
        "        self.truncation_seq_length = truncation_seq_length\n",
        "\n",
        "    def __call__(self, raw_batch: Sequence[Tuple[str, str]]):\n",
        "        # RoBERTa uses an eos token, while ESM-1 does not.\n",
        "        batch_size = len(raw_batch)\n",
        "        batch_labels, seq_str_list = zip(*raw_batch)\n",
        "        seq_encoded_list = [self.alphabet.encode(seq_str) for seq_str in seq_str_list]\n",
        "        if self.truncation_seq_length:\n",
        "            seq_encoded_list = [seq_str[:self.truncation_seq_length] for seq_str in seq_encoded_list]\n",
        "        max_len = max(len(seq_encoded) for seq_encoded in seq_encoded_list)\n",
        "        tokens = torch.empty(\n",
        "            (\n",
        "                batch_size,\n",
        "                max_len + int(self.alphabet.prepend_bos) + int(self.alphabet.append_eos),\n",
        "            ),\n",
        "            dtype=torch.int64,\n",
        "        )\n",
        "        tokens.fill_(self.alphabet.padding_idx)\n",
        "        labels = []\n",
        "\n",
        "        for i, (label, seq_str, seq_encoded) in enumerate(\n",
        "            zip(batch_labels, seq_str_list, seq_encoded_list)\n",
        "        ):\n",
        "            labels.append(label)\n",
        "            if self.alphabet.prepend_bos:\n",
        "                tokens[i, 0] = self.alphabet.cls_idx\n",
        "            seq = torch.tensor(seq_encoded, dtype=torch.int64)\n",
        "            tokens[\n",
        "                i,\n",
        "                int(self.alphabet.prepend_bos) : len(seq_encoded)\n",
        "                + int(self.alphabet.prepend_bos),\n",
        "            ] = seq\n",
        "            if self.alphabet.append_eos:\n",
        "                tokens[i, len(seq_encoded) + int(self.alphabet.prepend_bos)] = self.alphabet.eos_idx\n",
        "\n",
        "        return tokens, labels"
      ],
      "metadata": {
        "id": "zy0Nvoepijd_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Alphabet(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        standard_toks: Sequence[str],\n",
        "        prepend_toks: Sequence[str] = (\"<pad>\", \"<eos>\", \"<unk>\"),\n",
        "        append_toks: Sequence[str] = (\"<cls>\", \"<mask>\", \"<sep>\"),\n",
        "        prepend_bos: bool = True,\n",
        "        append_eos: bool = True,\n",
        "    ):\n",
        "        self.standard_toks = list(standard_toks)\n",
        "        self.prepend_toks = list(prepend_toks)\n",
        "        self.append_toks = list(append_toks)\n",
        "        self.prepend_bos = prepend_bos\n",
        "        self.append_eos = append_eos\n",
        "\n",
        "        self.all_toks = list(self.prepend_toks)\n",
        "        self.all_toks.extend(self.standard_toks)\n",
        "        self.all_toks.extend(self.append_toks)\n",
        "\n",
        "        self.tok_to_idx = {tok: i for i, tok in enumerate(self.all_toks)}\n",
        "        self.unk_idx = self.tok_to_idx[\"<unk>\"]\n",
        "        self.padding_idx = self.get_idx(\"<pad>\")\n",
        "        self.cls_idx = self.get_idx(\"<cls>\")\n",
        "        self.mask_idx = self.get_idx(\"<mask>\")\n",
        "        self.eos_idx = self.get_idx(\"<eos>\")\n",
        "        self.all_special_tokens = ['<eos>', '<pad>', '<mask>']\n",
        "        self.unique_no_split_tokens = self.all_toks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_toks)\n",
        "\n",
        "    def get_idx(self, tok):\n",
        "        return self.tok_to_idx.get(tok, self.unk_idx)\n",
        "\n",
        "    def get_tok(self, ind):\n",
        "        return self.all_toks[ind]\n",
        "\n",
        "    def to_dict(self):\n",
        "        return self.tok_to_idx.copy()\n",
        "\n",
        "    def get_batch_converter(self):\n",
        "        return BatchConverter(self)\n",
        "\n",
        "    def _tokenize(self, text) -> str:\n",
        "        return text.split()\n",
        "\n",
        "    def tokenize(self, text: str, **kwargs) -> List[str]:\n",
        "        tokens = []\n",
        "        i = 0\n",
        "        while i < len(text):\n",
        "            if text[i] == '<':\n",
        "                j = text.find('>', i)\n",
        "                if j == -1:\n",
        "                    raise ValueError(f\"Unclosed special token starting at position {i}: {text[i:i+10]}\")\n",
        "                tokens.append(text[i:j+1])\n",
        "                i = j + 1\n",
        "            else:\n",
        "                tokens.append(text[i])\n",
        "                i += 1\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.tok_to_idx[tok] for tok in self.tokenize(text)]"
      ],
      "metadata": {
        "id": "-MYq8KiXiK4x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Union\n",
        "\n",
        "from esm.modules import ESM1bLayerNorm, TransformerLayer\n",
        "\n",
        "class ESM2SI(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int = 33,\n",
        "        embed_dim: int = 1280,\n",
        "        attention_heads: int = 20,\n",
        "        alphabet: any = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.embed_dim = embed_dim\n",
        "        self.attention_heads = attention_heads\n",
        "\n",
        "        self.alphabet = alphabet\n",
        "        self.alphabet_size = len(alphabet)\n",
        "        self.padding_idx = alphabet.padding_idx\n",
        "        self.cls_idx = alphabet.cls_idx\n",
        "        self.eos_idx = alphabet.eos_idx\n",
        "        self.prepend_bos = alphabet.prepend_bos\n",
        "        self.append_eos = alphabet.append_eos\n",
        "\n",
        "        self._init_submodules()\n",
        "\n",
        "    def _init_submodules(self):\n",
        "        self.embed_scale = 1\n",
        "        self.embed_tokens = nn.Embedding(\n",
        "            self.alphabet_size,\n",
        "            self.embed_dim,\n",
        "            padding_idx=self.padding_idx,\n",
        "        )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerLayer(\n",
        "                    self.embed_dim,\n",
        "                    4 * self.embed_dim,\n",
        "                    self.attention_heads,\n",
        "                    add_bias_kv=False,\n",
        "                    use_esm1b_layer_norm=True,\n",
        "                    use_rotary_embeddings=True,\n",
        "                )\n",
        "                for _ in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.emb_layer_norm_after = ESM1bLayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(self, tokens, repr_layers=[], return_representation=True):\n",
        "        assert tokens.ndim == 2\n",
        "        padding_mask = tokens.eq(self.padding_idx)\n",
        "\n",
        "        x = self.embed_scale * self.embed_tokens(tokens)\n",
        "\n",
        "        # Apply padding mask\n",
        "        if padding_mask is not None:\n",
        "            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))\n",
        "\n",
        "        repr_layers = set(repr_layers)\n",
        "        hidden_representations = {}\n",
        "        if 0 in repr_layers:\n",
        "            hidden_representations[0] = x\n",
        "\n",
        "        x = x.transpose(0, 1)  # (B, T, E) -> (T, B, E)\n",
        "\n",
        "        if not padding_mask.any():\n",
        "            padding_mask = None\n",
        "\n",
        "        for layer_idx, layer in enumerate(self.layers):\n",
        "            x, _ = layer(\n",
        "                x,\n",
        "                self_attn_padding_mask=padding_mask,\n",
        "                need_head_weights=False,\n",
        "            )\n",
        "            if (layer_idx + 1) in repr_layers:\n",
        "                hidden_representations[layer_idx + 1] = x.transpose(0, 1)\n",
        "\n",
        "        x = self.emb_layer_norm_after(x)\n",
        "        x = x.transpose(0, 1)  # (T, B, E) -> (B, T, E)\n",
        "\n",
        "        if (layer_idx + 1) in repr_layers:\n",
        "            hidden_representations[layer_idx + 1] = x\n",
        "\n",
        "        if return_representation:\n",
        "            return {\"representations\": hidden_representations}\n",
        "        else:\n",
        "            return {}"
      ],
      "metadata": {
        "id": "PKxsQbLn1HZ4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ESM2_Linear(nn.Module):\n",
        "    def __init__(self, embed_dim, nodes):\n",
        "        super().__init__()\n",
        "        self.esm2 = ESM2SI(num_layers = 6,\n",
        "              embed_dim = 128,\n",
        "              attention_heads = 16,\n",
        "              alphabet = alphabet)\n",
        "\n",
        "        self.nodes = 40\n",
        "        self.dropout3 = 0.2\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, self.nodes)\n",
        "        self.dropout3 = nn.Dropout(self.dropout3)\n",
        "\n",
        "        self.output = nn.Linear(self.nodes, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, tokens, repr_layer=6, return_representation=True):\n",
        "        x = self.esm2(tokens, repr_layers=[repr_layer], return_representation=return_representation)\n",
        "\n",
        "        # get representation\n",
        "        x = x[\"representations\"][repr_layer][:, 0]\n",
        "        x_o = x.unsqueeze(2)\n",
        "        x = self.flatten(x_o)\n",
        "\n",
        "        # prediction head\n",
        "        x = self.fc(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "64ov_YvH4Yvv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = Alphabet(standard_toks = 'AGCT')\n",
        "model = ESM2_Linear(128, 40).to(device)\n",
        "modelfile = '/content/ESM2SI_3.1_fiveSpeciesCao_6layers_16heads_128embedsize_4096batchToks_MLMLossMin.pkl'\n",
        "state_dict = torch.load(modelfile, map_location=device)\n",
        "new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "model.esm2.load_state_dict(new_state_dict, strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTDkLEUa_PqN",
        "outputId": "eafd5d73-bb76-4a15-c135-a430460b0e68"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=[], unexpected_keys=['contact_head.regression.weight', 'contact_head.regression.bias', 'lm_head.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'supervised_linear.weight', 'supervised_linear.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/Muscle_dataset.csv')\n",
        "obj_col = 'te_log'\n",
        "seq_type = 'utr'\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "1Rf0SrENztE_"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, val_data = train_test_split(\n",
        "    data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "aoMxHU8krVmH"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = FastaBatchedDataset(train_data.loc[:,obj_col], train_data[seq_type])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                        batch_size= batch_size,\n",
        "                                        shuffle = True,\n",
        "                                        collate_fn=alphabet.get_batch_converter())\n",
        "\n",
        "val_dataset = FastaBatchedDataset(val_data.loc[:,obj_col], val_data[seq_type])\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                        batch_size= batch_size,\n",
        "                                        shuffle = False,\n",
        "                                        collate_fn=alphabet.get_batch_converter())"
      ],
      "metadata": {
        "id": "TbvMspif5jw9"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "def setup_optimizer_and_scheduler(model, train_loader, epochs, batch_size, lr_for_esm=1e-5, lr_for_head=1e-3):\n",
        "    \"\"\"\n",
        "    Set up optimizer and scheduler for ESM2 finetuning model.\n",
        "\n",
        "    Args:\n",
        "        model: The model containing esm2 encoder and regression head (like fc/output layers).\n",
        "        train_loader: DataLoader for training set.\n",
        "        epochs: Total number of epochs.\n",
        "        batch_size: Training batch size.\n",
        "        lr_for_esm: Learning rate for pretrained ESM2 encoder.\n",
        "        lr_for_head: Learning rate for new prediction head.\n",
        "\n",
        "    Returns:\n",
        "        optimizer, scheduler\n",
        "    \"\"\"\n",
        "\n",
        "    # Define scaled learning rates\n",
        "    lr_esm = batch_size * lr_for_esm\n",
        "    lr_head = batch_size * lr_for_head\n",
        "\n",
        "    # Collect parameters\n",
        "    esm2_params = list(model.esm2.parameters())\n",
        "    head_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not name.startswith(\"esm2.\"):  # Everything not inside esm2 is considered head\n",
        "            head_params.append(param)\n",
        "\n",
        "    # Group parameters\n",
        "    param_groups = [\n",
        "        {'params': esm2_params, 'lr': lr_esm},\n",
        "        {'params': head_params, 'lr': lr_head}\n",
        "    ]\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = AdamW(param_groups, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
        "\n",
        "    # Initialize scheduler\n",
        "    scheduler = OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=[lr_esm, lr_head],\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        epochs=epochs,\n",
        "    )\n",
        "\n",
        "    return optimizer, scheduler\n"
      ],
      "metadata": {
        "id": "KJbCOYLYTSlU"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer, scheduler = setup_optimizer_and_scheduler(model, train_dataloader, 10, 32)"
      ],
      "metadata": {
        "id": "wFIVhls5Tbqe"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "\n",
        "scaler = GradScaler('cuda')\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(inputs)\n",
        "            loss = criterion(predictions, labels.view(-1, 1))\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in train_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "            with autocast('cuda'):\n",
        "                predictions = model(inputs)\n",
        "                loss = criterion(predictions, labels.view(-1, 1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.squeeze(all_preds)\n",
        "    all_labels = np.squeeze(all_labels)\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        f\"loss\": total_loss / len(loader),\n",
        "        f\"spearman\": spearmanr(all_labels, all_preds)[0],\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# def predict(model, dataframe, config,device):\n",
        "#     model.eval()\n",
        "#     dataset = ProteinDataset(dataframe)\n",
        "#     loader = DataLoader(dataset, batch_size=8, num_workers=config['num_workers'],shuffle=False)\n",
        "\n",
        "\n",
        "#     all_preds = []\n",
        "#     with torch.no_grad():\n",
        "#         for batch in loader:\n",
        "#             batch = [b.to(device) if isinstance(b, torch.Tensor) else b for b in batch]\n",
        "#             preds = model(*batch[:-1])  # Exclude labels\n",
        "#             all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "#     predicts = np.squeeze(all_preds)\n",
        "\n",
        "#     return predicts\n"
      ],
      "metadata": {
        "id": "sFBS0ii7Dcc1"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.HuberLoss()\n",
        "early_stop_patience = 5"
      ],
      "metadata": {
        "id": "NXP3-3CAFp5y"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_metric = -float('inf')\n",
        "epochs_since_improvement = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "        train_loss = train_one_epoch(model, train_dataloader, optimizer, criterion, device)\n",
        "        val_metric = evaluate(model,val_dataloader, criterion, device)\n",
        "        print(f\"[Epoch {epoch}] Train Loss: {train_loss:.4f} | Val loss:{val_metric['loss']:.4f} | Val Spearman: {val_metric['spearman']:.4f}\")\n",
        "\n",
        "        if val_metric['spearman'] > best_val_metric:\n",
        "            best_val_metric = val_metric['spearman']\n",
        "            # torch.save(model.state_dict(), os.path.join(save_dir, f\"best_model_fold{fold + 1}.pt\"))\n",
        "            # print(f\"New best Spearman: {best_val_metric:.4f} — Model saved.\")\n",
        "            epochs_since_improvement = 0\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "\n",
        "        if epochs_since_improvement >= early_stop_patience:\n",
        "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "print(\"Best_Spearman: {best_val_metric:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrXGGSlIGZ1N",
        "outputId": "8f2d8b7a-a7ea-412e-96f1-acaa8f0a3cdf"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0] Train Loss: 0.5028 | Val loss:1.6197 | Val Spearman: 0.6351\n",
            "[Epoch 1] Train Loss: 0.5002 | Val loss:1.5547 | Val Spearman: 0.6703\n",
            "[Epoch 2] Train Loss: 0.4819 | Val loss:1.5486 | Val Spearman: 0.6828\n",
            "[Epoch 3] Train Loss: 0.4961 | Val loss:1.4674 | Val Spearman: 0.7293\n",
            "[Epoch 4] Train Loss: 0.4575 | Val loss:1.3269 | Val Spearman: 0.7929\n",
            "[Epoch 5] Train Loss: 0.4037 | Val loss:1.0055 | Val Spearman: 0.8298\n",
            "[Epoch 6] Train Loss: 0.3449 | Val loss:1.0599 | Val Spearman: 0.8530\n",
            "[Epoch 7] Train Loss: 0.3048 | Val loss:0.8797 | Val Spearman: 0.8641\n",
            "[Epoch 8] Train Loss: 0.2838 | Val loss:0.8158 | Val Spearman: 0.8758\n",
            "[Epoch 9] Train Loss: 0.2686 | Val loss:0.8039 | Val Spearman: 0.8768\n",
            "Best_Spearman: {best_val_metric:.4f}\n"
          ]
        }
      ]
    }
  ]
}